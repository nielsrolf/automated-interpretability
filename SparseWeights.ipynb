{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: colab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "DEVELOPMENT_MODE = False\n",
    "# Detect if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install if in Colab\n",
    "if IN_COLAB:\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    # Install a faster Node version\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
    "\n",
    "# Hot reload in development mode & not running on the CD\n",
    "if not IN_COLAB:\n",
    "    from IPython import get_ipython\n",
    "    ip = get_ipython()\n",
    "    if not ip.extension_manager.loaded:\n",
    "        ip.extension_manager.load('autoreload')\n",
    "        %autoreload 2\n",
    "        \n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "IN_GITHUB = True\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")\n",
    "\n",
    "\n",
    "\n",
    "import circuitsvis as cv\n",
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "torch.set_grad_enabled(False)\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "device = utils.get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "gpt2_tokens = model.to_tokens(gpt2_text)\n",
    "print(gpt2_tokens.device)\n",
    "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assuming sparse feature interactions\n",
    "\n",
    "Recent work has shown evidence for the hypothesis that the residual stream of a GPT model holds a\n",
    "large set of sparse interpretable features that are in superposition.\n",
    "It seems reasonable to assume that if this is the case, the operations done by operations that read\n",
    "from and write into the residual stream can similarly be described in this unknown sparse feature\n",
    "space.\n",
    "\n",
    "In this notebook, I want to explore if we can reformulate a pretrained GPT-model into one that\n",
    "explicitly computes in such an assumed sparse feature space, and recover interpretable features by\n",
    "enforcing sparse feature interactions.\n",
    "\n",
    "## Notation\n",
    "\n",
    "X_{token, layer} : (d)-dimensional vector: residual stream activation. I will sometimes use x for a\n",
    "specific X_{t,l} - then t,l are assumed to be some arbitrary constant token and layer indices.\n",
    "\n",
    "f(x) = (f_1(x), ... f_D(x)): D-dimensional vector of unkown sparse interpretable features\n",
    "\n",
    "x = D f(x) is the superposition assumption, where:\n",
    "D: (d, D) is the dictionary of feature representatives, where the i-th column is the representative of\n",
    "feature i\n",
    "\n",
    "\n",
    "\n",
    "## Problem formulation\n",
    "\n",
    "For a given GPT, we want to reformulate the MLP-layers and the attention layers, such that:\n",
    "\n",
    "MLP'(x) = a(W' f(x))\n",
    "where:\n",
    "    - a(.) is a nonlinear activation function\n",
    "    - W' is sparse\n",
    "    - D MLP'(x) ~= MLP(x) : the new MLP approximates the original MLP\n",
    "\n",
    "And:\n",
    "SelfAttn(X) = x W_q X W_w.T / sqrt(d) X W_v = A X W_v\n",
    "SelfAttn'(f(X)) = A f(x) W_v'\n",
    "\n",
    "\n",
    "\n",
    "## GPT-2-small architecture\n",
    "```python\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_mid:\", normalized_resid_mid.shape)\n",
    "        pre = einsum(\"batch position d_model, d_model d_mlp -> batch position d_mlp\", normalized_resid_mid, self.W_in) + self.b_in\n",
    "        post = gelu_new(pre)\n",
    "        mlp_out = einsum(\"batch position d_mlp, d_mlp d_model -> batch position d_model\", post, self.W_out) + self.b_out\n",
    "        return mlp_out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "\n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post\n",
    "```\n",
    "\n",
    "\n",
    "## Can we \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "\n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_pre:\", normalized_resid_pre.shape)\n",
    "\n",
    "        q = einsum(\"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "        k = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_K) + self.b_K\n",
    "\n",
    "        attn_scores = einsum(\"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\", q, k)\n",
    "        attn_scores = attn_scores / math.sqrt(self.cfg.d_head)\n",
    "        attn_scores = self.apply_causal_mask(attn_scores)\n",
    "\n",
    "        pattern = attn_scores.softmax(dim=-1) # [batch, n_head, query_pos, key_pos]\n",
    "\n",
    "        v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_V) + self.b_V\n",
    "\n",
    "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", pattern, v)\n",
    "\n",
    "        attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_O) + self.b_O\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.W_en = nn.Parameter(torch.empty((cfg.d_model, cfg.d_sparse)))\n",
    "        nn.init.normal_(self.W_en, std=self.cfg.init_range)\n",
    "        self.b_dec = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros((cfg.d_sparse)))\n",
    "        self.W_dec = nn.Parameter(torch.empty((cfg.d_sparse, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # x: [batch, position, d_model]\n",
    "        # y: [batch, position, d_sparse]\n",
    "        x = x - self.b_dec\n",
    "        pre = einsum(\"batch position d_model, d_model d_mlp -> batch position d_mlp\", x, self.W_enc) + self.b_enc\n",
    "        f = gelu_new(pre)\n",
    "        return f\n",
    "    \n",
    "    def decode(self, f):\n",
    "        pre = einsum(\"batch position d_model, d_model d_mlp -> batch position d_mlp\", f, self.W_dec) + self.b_dec\n",
    "        x = gelu_new(pre)\n",
    "        return x\n",
    "    \n",
    "        \n",
    "class SparseTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "        \n",
    "        self.sparse_loss_w = 0.1\n",
    "\n",
    "\n",
    "        self.sparse_attn = SparseAttention([\n",
    "            self.ln1,\n",
    "            self.attn\n",
    "        ])\n",
    "        self.sparse_mlp = SparseMLP(\n",
    "            self.ln2\n",
    "            self.mlp\n",
    "        )\n",
    "\n",
    "    def forward(self, dense_resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(dense_resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = dense_resid_pre + attn_out\n",
    "\n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post\n",
    "    \n",
    "    def encode_forward_decode(self, dense_resid_pre):\n",
    "        sparse_resid_pre = self.decode(dense_resid_pre)\n",
    "        sparse_resid_post = self.forward_sparse(sparse_resid_pre)\n",
    "        dense_resid_post = self.encode(sparse_resid_post)\n",
    "        return dense_resid_post\n",
    "    \n",
    "    def loss(self, dense_resid_pre):\n",
    "        # Gt forward     \n",
    "        normalized_resid_pre = self.ln1(dense_resid_pre)\n",
    "        pattern, attn_out = self.sparse_attn.attn_pattern(normalized_resid_pre)\n",
    "        resid_mid = dense_resid_pre + attn_out\n",
    "        \n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        \n",
    "        # Sparse forward\n",
    "        sparse_resid_pre = self.encode(dense_resid_pre)\n",
    "        sparse_resid_mid = self.sparse_attn(pattern, sparse_resid_pre)\n",
    "        sparse_resid_post = self.sparse_mlp(sparse_resid_mid)\n",
    "        dense_resid_post = self.decode(sparse_resid_post)\n",
    "        \n",
    "        # loss components\n",
    "        ## sparsity\n",
    "        loss_f_sparse = l1(sparse_resid_pre)\n",
    "        loss_mlp_sparse = l1(self.sparse_mlp.W)\n",
    "        loss_attn_sparse = l1(self.W_V_sparse)\n",
    "        # We could additionally try to decompose the attention pattern computation, but we keep this\n",
    "        # for another time\n",
    "        ## reconstruction\n",
    "        mse_post = mse(\n",
    "            resid_post,\n",
    "            dense_resid_post\n",
    "        )\n",
    "        \n",
    "        loss = loss_f_sparse + loss_mlp_sparse + loss_attn_sparse + mse_post\n",
    "        return dense_resid_post\n",
    "\n",
    "\n",
    "class SparseMLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_mid:\", normalized_resid_mid.shape)\n",
    "        pre = einsum(\"batch position d_model, d_model d_mlp -> batch position d_mlp\", normalized_resid_mid, self.W_in) + self.b_in\n",
    "        post = gelu_new(pre)\n",
    "        mlp_out = einsum(\"batch position d_mlp, d_mlp d_model -> batch position d_model\", post, self.W_out) + self.b_out\n",
    "        return mlp_out\n",
    "    \n",
    "    \n",
    "class SparseAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_sparse, cfg.d_head )))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "\n",
    "    def attn_pattern(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_pre:\", normalized_resid_pre.shape)\n",
    "\n",
    "        q = einsum(\"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "        k = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_K) + self.b_K\n",
    "\n",
    "        attn_scores = einsum(\"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\", q, k)\n",
    "        attn_scores = attn_scores / math.sqrt(self.cfg.d_head)\n",
    "        attn_scores = self.apply_causal_mask(attn_scores)\n",
    "\n",
    "        pattern = attn_scores.softmax(dim=-1) # [batch, n_head, query_pos, key_pos]\n",
    "        return pattern\n",
    "    \n",
    "    def forward(self, pattern, normalized_resid_pre):\n",
    "        v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_V) + self.b_V\n",
    "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", pattern, v)\n",
    "        attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_O) + self.b_O\n",
    "        return pattern, attn_out\n",
    "    \n",
    "    def forward_sparse(self, x, pattern):\n",
    "        v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", x, self.W_V_sparse) + self.b_V\n",
    "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", pattern, v)\n",
    "        return z\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores\n",
    "\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
